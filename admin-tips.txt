apt-get update

cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
br_netfilter
EOF

cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
sudo sysctl --system

apt-get install -y docker

sudo apt-get install -y apt-transport-https ca-certificates curl

-- check the current config file content.
-- including clusters, contexts, users, current context, preferences etc. 
kubectl config view 

-- cluster management, cluster is a combination of control plane and a set of working node, plus the etcd for parameter storage
1. check cluster information
kubectl config set <key=value> -- set individual key value
kubectl config get-clusters
kubectl config set-cluster --add a new cluster in kubeconfig
kubectl config delete-cluster <cluster>

-- context management, context is setup of default cluster, user, and namespace when no specific set
kubectl config current-context --- display current context
kubectl config get-contexts
kubectl config use-context <context>
kubectl config set-context  --- add a new context
kubectl config delete-context <context> -- delete a context

-- User management
kubectl config get-users
kubectl config set-credentials <add a new user>
kubectl config delete-user <user>

-- create deployment
kubectl create deployment <dep-name> --image=<image id>
-- get the deployment
kubectl get deployments.apps
kubectl get pod
kubectl describe deployments.apps <dep-name> -- you'll find out the container name

-- upgrade the deployment image
kubectl set image <dep-name> <container-name>=<new image id> --record
kubectl describe deployments.apps <dep-name> | grep -i | image

-- upgrade the kubeadm/kubelet for master and worker nodes
1. From master
1.0 check current version
kubeadm version
1.1 check the available version, also include the current version
kubeadm upgrade plan --- this will print out the available version of kubeadm and kubelet, this can only run on master
1.2 upgrade the executable (kubeadm and kubelet)
apt update
apt-mark unhold kubeadm, kubelet
apt install kubeadm=<version> // pls note the version structure: <major>.<minor>.<revision>-<build number: 00>
1.3 apply the upgrade change
-- download the corresponding docker images for the version and label it accordingly.
as folllowing instruction. 
-- apply the version.
kubeadm upgrade apply v<version number> --- please note, this version doesn't include build number (-xx)
--- this will download the required docker images from gcr.io, which doesn't work in China without a VPN, so need to be carefule
1.4 upgrade the CNI plugin when necessary
2. From other control nodes
<download and upgrade the executable>
kubeadm upgrade plan -- check the upgrade availabilty
kubeadm upgrade node --- no need to provide the version, instead of kubeadm upgrade apply
3. Drain the workload from master nodes
kubectl drain <node to drain > --ignore-daemonsets
4. upgrade the kubelet & kubectl version
apt-mark unhold kubelet && apt-get upgrade && apt-get install -y kubelet=<version-00> && apt-mark hold kubelet
apt-mark unhold kubectl && apt-get upgrade && apt-get install -y kubectl=<version-00> && apt-mark hold kubectl
systemctl daemon-reload
systemctl restart kubelet
5. Uncordon the workload
kubectl uncordon <node-to-drain>

--For work nodes
1.upgrade the kubeadm executable
apt-mark unhold kubeamd && apt-get update && apt-get install -y kubeadm=<version-xx> && apt-mark hold kubeadm
2. upgrade the node
kubeadm upgrade node
3. drain the node 
from master
kubectl drain <node to drain>
4. upgrade the kubelet
apt-mark unhold kubelet && apt-get update && apt-get install -y kubelet=<version-xx> && apt-mark hold kubelet
5. reload the services
systemctl daemon-reload
systecmtl restart kubelet
6. uncordon the node
from master
kubectl uncordon <node to drain>

verify the status
from master
kubectl get nodes

Static Pod - static pod will be tied to a specific node, and will NOT be managed by the apiserver
- static pod will automatically restart when kubelet restart
two ways to run static pod
- Filesystem-hosted solution:
kubelet will load yaml files (manefist) from a folder in filesystem in the config file or the manifest path
there are two options for this:
1. by setting in kubelet config file: usually /var/lib/kubelet/config.yaml, then staticPodPath: 
--- the config file is specified in the kubelet command --config, or through
1.1 to identify the config file path, using
ps -aux | grep kubelet, and then find out the path of the --config
1.2 then grep the path by:
cat <config file path from above> | grep -i static
1.3 then create the manifest file to the folder
2. by setting a pod-manifest-path to kubelet. this is not included by default, so if you want to change, need to restart kubelet on the node
--- set the kubelet with --pod-manifest-path parameter
- Web-hosted static pod manifest
kubelet will periodically scan the URL to pick up the manifests from the url. this is done by specify a url to kubelet command
kubelet --manifest-url=<url>

ETCD cluster management
- Run a single-node cluster
etcd --listen-client-urls=http://<private-ip>:2379 --advertise-client-urls=http://<private-ip>:2379
--- then run the APIServer with --etcd-servers=<private-ip>:2379

- Run a multi-nodes cluster
etcd --listen-client-urls=http://<ip1>:2379,http://<ip2>:2379 --advertise-client-urls=http:<ip1>:2379,http:<ip2>:2379
--- then run the APIServer with --etcd-servers=<ip1>:2379,<ip2>:2379

- Run a multi-node cluster with Load-balancer
run the etcd with all the IPs behind the load-balancer
APIServer --listen-client-urls=<lb-ip>:2379

- Secure the ETCD
this includes two parts:
1. peer comms - between ETCD members
1.1 specify --peer-key-file=<peer.key>, --peer-cert-file=<peer.cert> and use HTTPS as comms protocol
2. client comms - between ETCD and the clients (API server in this case, or etcdctl in some case)
3. set trusted CA for client access
--client-cert-auth=<true/false>, this will force the ETCD cluster verify client using system CA or CA passed by trusted-ca-file
--trusted-ca-file=<CA file which signes the client cert>

find out the secure information:
1. findout the etcd pod by:
kubectl get pods -n kube-system
2. describe the pod by:
kubectl describe pod/<etc pod>
3. find out the cert/key and trust ca file as well as the end-points
4. then operate with etcdctl by:
ETCDCTL_API=3 etcdctl --endpoints=<one from listen-client-urls> --cacert=<trusted-ca-file> --cert=<cert-file> --key=<key file> snapshot save <filename>
ETCDCTL_API=3 etcdctl --endpoints=<one from listen-client-urls> --cacert=<trusted-ca-file> --cert=<cert-file> --key=<key file> restore <filename>

Delete resource
kubectl delete pod, service <names> --grace-period=<seconds, -1, 0, 1 (immediate)>, --force (grace-period need to 0)

service
1. create service by expose
1.1 kubectl run pod-web --image=nginx
1.2 kubectl expose pod pod-web --name=web-pod-svc --port=80 --- this will create a svc of default clusterIP type.
1.3 lookup service
--- kubectl exec -it <pod-name> -- nslookup <svc-name> ---- this will lookup by service name
--- kubectl exec -it <pod-name> -- nslookup <svc-end-point-ip, and replace . by ->.default.pod ---- this will lookup the default pod
you can find out the IP address by kubectl get pod -o wide
you can't directly ping to a pod, you have to ping the ip, or the service wrapping the pod. 

1.4 check dns status
--- install dnsutil jessie-dnsutils
first install the docker image, as I'm installing from docker hub, so I need to change the image setting below
then apply the dnsutil pod by 


apiVersion: v1
kind: Pod
metadata:
  name: dnsutils
  namespace: default
spec:
  containers:
  - name: dnsutils
    image: claudiubelu/jessie-dnsutils:1.3
    command:
      - sleep
      - "3600"
    imagePullPolicy: IfNotPresent
  restartPolicy: Always

then can run the nslookup using the newly created pod
-- kubectl exec -it dnsutils -- nslookup svc-name
-- kubectl exec -it dnsutils -- nslookup kubernetes.default  --- this should always work
can also print the local dns setting by dump the local names resolution:
-- kubectl exec -it dnsutils -- cat /etc/resolv.conf
check if DNS pod is running
-- kubectl get pods -n kube-system -l k8s-app=kube-dns
check for DNS logs
-- kubectl logs -n kube-system -l k8s-app=kube-dns

storage
- pv, two ways of provisions of pv
1. static
-- the yaml file:

apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv0003
spec:
  storageClassName: manual
  capacity:
    storage: 5Gi
  accessModes:
    - ReadWriteMany
  hostpath:
    path: /root/k8s/pv

a storageClassName is used to bind the PV to a class, which will later be used by PVC for PV provision. 
a PV without a class can only be used to bind PV to a PVC without class associated, or the dyanamic provision is disabled

the persistent volume claim 

--yaml file
apiVersion: v1
kind: PersistentVolumeClaim
metadata: 
  name: my-PVC
spec:
  storageClassName: manual -- align with previous setting
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 3Gi

apply the PV or PVC in pod
add a volumes section to specs of the pod

volumes:
- name: <volume name>
  persistentVolumeClaim:
    claimName: <the PVC name> 
- name: configmap
  configMap:
     name: configMap-name

then in the container's section, add volumeMounts section:
volumeMounts:
- name: <volume name defined in volumes>
  mountPath: <the path in the container>
  subPath: <subpath in local, if the same volume are to be used in multiple volumeMount>

you can also create a volume with configMap, which will map a configMap to a file. 

2. dynamic
- dynamic provision is provided through PVC with a storage class (storageClassName), when a PVC doesn't have storage class, 
dynamic provision is disabled. 
- To enable, cluster admin need to enable DefaultStorageClass admission controller on API Server. 
--- --enable-admission-plugins 

- Create pod with replicas, use deployment than directly run command
kubectl create deployment <dept-name> --image=<imageid> --replica=3
kubectl run <pod-name> --image=<imageid> --- this will create a pod with only one replica
kubectl get deployments.apps --- will get the apps.
kubectl get pods --- this will get the pods

- scheduling
1. drain command with kubectl drain <node> --ignore-daemonsets --- this will drain workload off the node, and the node will become unscheduleable
2. uncordon the node kubectl uncordon <node> --- this will make the node schedulable again
3. tain the node with NoSchedule, the node will be unschedule for workload without appropriate toleranace. 
-- kubectl taint <node> <key>=<value>:NoScheule/PreferNoSchedule/NoExecute
4. Tolerate the node for scheduling
-- add "tolerations" secion to the spec, with key, oprator, value, and effect

debug:
1. kubectl describe pod/svc/deployment/... 
2. kubectl logs pod/pod-name
3. systemctl status kubelet
4. journalctl -f -t kubelet

CSR
1. the CSR yaml
apiVersion: v1
kind: CertificateSigningRequest
metadata:
  name: myuser-csr
spec:
  request: <the request >
  signerName: kubernetes.io/kube-apiserver-client
  experirationSeconds: 86400 # one day
  usages:
  - client auth
2. request is the base64 string of the csr. 
cat myuser.csr | base64 | tr -d "\n"
3. approve the CSR
kubectl certificate approve <my-csr>
4. get/retrieve the cert from the approved CSR - it's a base64-encoded in the .status.certificate field:
kubetl get csr/my-csr -o jsonpath='{.status.certificate}' | base64 -d > myuser.crt 

User:
1. create user with cert/key:
kubectl config set-credentials --client-certificate=<cert-file> --client-key=<key-file>
2. create user with bearer token flags:
kubectl config set-credentials --token=<bearer-token>
3. create user with basic auth flags:
kubectl config set-credentials --username=<user-name> --password=<password>
4. using an external auth provider:
kubectl config set-credentials cluster-admin --auth-provider=<gcp, aws etc>
5. Enable OpenID connect auth provider for cluster-admin, usually the service will provide openid, clientid, client-secret, refresh-token
kubectl config set-credentials cluster-admin \
  --auth-provider=oidc 
  --auth-provider-arg=issuer-url=<issurer-url> \
  --auth-provider-arg=client-id=<client-id> \
  --auth-provider-arg=id-token=<id_token> \
  --auth-provider-arg=client-secret=<client secret> \
  --auth-provider-arg=refresh-token=<refresh token> \
  --auth-provider-arg=idp-certificate-authority=<path to the CA cert>
6. Using --token option:
kubectl --token=<bearer token> commands
7. Using WebHook Token Authentication
webhook auth is a hook for verifying bearer tokens

scheduling
- kubectl cordon node-name -- if you want to drain a node, need to first make it unschedulable
kubectl cordon <node0name>
- drain the pods from the node --- need to ignore daemonsets, and clean the local data and force
kubectl drain <node> --ignore-daemonsets --delete-local-data --force
- need to check node
kubectl get nodes
- uncordon the node
kubectl uncordon <node-name>
then get the nodes

upgrade the kubeadm
1. kubectl get nodes
2. kubectl cordon <master-node>
3. kubectl drain <master-node> --ignore-daemonsets --force
4. apt-mark unhold kubeadm kubelet kubectl
5. apt-get update & apt-get install -y kubeadm=1.20.1-00 kubelet=1.20.1-00 kubectl=1.20.1-00
6. apt-mark hold kubeadm kubectl kubelet
7. kubeadm upgrade plan
8. kubeadm upgrade apply <v1.20.1> --etcd-upgrade=false
9. systemctl restart kubelet, systemctl daemon-reload
10. kubectl uncordon <master-node>

Network policy
apiVersion: v1
metadata:
  name: policy name
spec:
- application scope:
  podSelector - select a group of pods to apply the policy.
- policyTypes - Ingress or Egress or both. 
  policyTypes:
  - Ingress and/or
  - Egress
- ingress - define the rules that will apply to the incoming traffic of the selected pods. 
  - from: -- can have multiple from, each represent a rule
          -- each rule include:
          ---- ipBlock: (the cidr), this is external-cluster IP range. 
          ---- podSelector: (matchLabels)
          ---- nameSpaceSelector: (matchLabels)
          ---- or combined:
               -namespaceSelector: 
                podSelector:
          ---- ports: (the type(UDP/TCP), the port, is an array)
               - protocol: TCP/UDP
                 port: the port or starting port
                 endPod: the end port of the range. 
               - protocol: TCP/UDP
                 port: 
  
- egress - defined the rules that will apply to the outbound traffic of the selected pods. 
  an array of rules that will apply to the traffic going out of the pods. it includes the following fields:
  - ports: an array (port, endport, and protocol)
  - to: include --- can have multiple -to. 
        - ipBlock (the cidr)
  - podSelector: apply to the pod in this namespace as this network policy. 
     matchLabels: 
        <labels>
  - namespaceSelector: namespace selector apply to All namespaces in the cluster 
     matchLabels: 
        <labels>
  - podSelector and namespaceSelector can join together by: 
  -namespaceSelector -- namespaceSelector first then podSelector 
   podSelector
   -- this will select pods with label from the selected namespaces. 

  please note: all namespace has a default label is: name: the-name. 

kubectl label <pod/pod-name> label-key=value --- this will add a new label
kubectl label pod/pod-name label-key-  --- this will delete a label

This can used to test the network policy - by using busybox. 
create a pod and with an TTY, which will automatically delete the pod after exit. 
kubectl run busybox --rm -ti --image:ubuntu -- /bin/sh
kubectl run busybox --rm -ti --image:ubuntu -n test --labels label=value,label2=value -- /bin/sh

Run a pod and also expose it as a service
kubectl run nginx --image:nginx --expose --port 80 -n namespace-test --labels label1=val,label2=val

make sure you check the indicated context from the exam, and change it as requested. 

Performance:
kubectl top node|pod -l|--selector <label> -A|--all-namespaces --sort-by=[cpu|memory]
kubectl top 

-- set the image repository to aliyun mirror
sudo curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg
apt-key add /usr/share/keyrings/kubernetes-archive-keyring.gpg
tee "deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main" >> /etc/apt/sources.list.d/kubernetes.list

apt-get update

-- install the kubeadm and kubelet and kubectl
sudo apt-get install -y kubelet kubeadm kubectl
or sudo apt install kubelet=<version>
-- get installed packages
apt list --installed | grep <packge-name>
-- hold package from update
sudo apt-mark hold kubelet kubeadm kubectl
-- apt update, --- update the available packages


--- set the CGroup Driver
sudo mkdir /etc/docker
cat <<EOF | sudo tee /etc/docker/daemon.json
{
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m"
  },
  "storage-driver": "overlay2"
}
EOF

sudo systemctl enable docker
sudo systemctl daemon-reload
sudo systemctl restart docker

--check the needed docker images for kubeadm
kubeadm config images list
-- result will be something like:
k8s.gcr.io/kube-apiserver:v1.23.1
k8s.gcr.io/kube-controller-manager:v1.23.1
k8s.gcr.io/kube-scheduler:v1.23.1
k8s.gcr.io/kube-proxy:v1.23.1
k8s.gcr.io/pause:3.6
k8s.gcr.io/etcd:3.5.1-0
k8s.gcr.io/coredns/coredns:v1.8.6

-- replace with the equivalent in docker hub
docker login

docker pull 
docker pull k8simage/kube-apiserver:v1.23.1
docker pull k8simage/kube-controller-manager:v1.23.1
docker pull k8simage/kube-scheduler:v1.23.1
docker pull k8simage/kube-proxy:v1.23.1
docker pull k8simage/pause:3.6
docker pull k8simage/etcd:3.5.1-0
docker pull 274658950/coredns:v1.8.6 -- this is because k8simage rep does't has the version.

-- add the tag that'll be recognized by kubeadm
docker tag k8simage/kube-apiserver:v1.23.1 k8s.gcr.io/kube-apiserver:v1.23.1
docker tag k8simage/kube-controller-manager:v1.23.1 k8s.gcr.io/kube-controller-manager:v1.23.1
docker tag k8simage/kube-scheduler:v1.23.1 k8s.gcr.io/kube-scheduler:v1.23.1
docker tag k8simage/kube-proxy:v1.23.1 k8s.gcr.io/kube-proxy:v1.23.1
docker tag k8simage/pause:3.6 k8s.gcr.io/pause:3.6
docker tag k8simage/etcd:3.5.1-0 k8s.gcr.io/etcd:3.5.1-0
docker tag 274658950/coredns:v1.8.6 k8s.gcr.io/coredns/coredns:v1.8.6

-- edit the default image repository, you can change the default configmap to point to a different image repository
1. check the current setting:
kubectl describe cm kubeadm-config -n kube-system
2. check the change the imageRepository: k8s.gcr.io, to a workable image repository
3. you can save the file in yaml format by:
kubectl get cm kubeadm-config -n kube-system -o yaml > kubeadmin-config.yaml
4. then edit and re-apply the yaml
-- now start the kubeadm
kubeadm init

It will now show the command for other node to join the cluster.

--- run command to get the tokens
kubeadm token list


-- print the join command
kubeadm token create --print-join-command

-- or if you want to do the detail work, first:
1. get the discovery token ca-cert hash string
openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //'
2. then issue the join command as:
kubeadm join --discovery-t0ken <token-name from token list command> --discovery-token-ca-cert-hash sha256:<the hash string>

-- alternatively, you can join by using the following command, if the token doesn't have CA pinned
kubeadm <endpoint url> --token <the token from the kubeadm token list> --discovery-token-unsafe-skip-ca-verification

-- install the same docker images on each node. 

-- once you got the token, you should also get the endpoint of the master. you then join to the master from the working node
kubeadm join 

-- reset the environment , this can be done when the environment is having problem, and can't successfully init.
kubeadm reset

-- set up the Kubernetes environment
export KUBECONFIG=/etc/kubernetes/admin.conf

-- troubleshooting
1. show status of the system pods
kubectl get pods -n kube-system -o wide
-- if any of the pods status isn't in Running status, then you can check the error by

2. show pod error status
kubectl describe pod <the node name> -n kube-system

3. show logs of the pod
kubectl logs <pod name> -n kube-system

4. show system journals with the kubernetes
journalctl -f -u kubelet
this will use tailed format of the logs to kubelet (like tail -f)
-f: follow, follow the latest journal
-n: show n lines
-u: show only logs for a service
--since: "yyyy-mm-dd hh:MM:ss" --until ""
--since 1 hour ago

4. after fixed the problems
systemctl daemon-reload
systemctl restart docker
systemctl restart kubelet (or systemctl restart kubelet.service)
systemctl status kubelet

-- set container/pod security context
kubectl run <pod-name> --image=<image> --dry-run=client -o yaml 
then change the yaml file and add the securityContext section in spec
securityContext:
  runAsUser: <userid>
  fsGroup: <groupid>

-- scheduling
1. mark the node unschedulable
kubectl cordon <nodename>
kubelctl drain <nodename> --force --ignore-daemonsets   --- this will force the pods/workload to be evicated from the node
2. mark the node scheduleable again
kubectl uncordon <nodename>
3. Add taint infor for node to unschedulable for certain attributes
kubectl taint node <nodename> key=value:effect -- this will make the node unschedule only if the pod has the explicit tolerance
--effect here can be: NoSchedule, PreferNoSchedule, NoExecute
kubectl taint node <nodename> key:effect -- this will add a taint with no value. 

currently taint can only apply to Node 

4. Remove the taint
kubectl taint node <nodename> key:effect-  -- this will clear the taint (the key and the effect), and the node will be schedule for all workloads
kubectl taint node <nodename> key-  --- this will clear all taint with the key. 

5. Create tolerance, a tolerance will all a tainted node to accept a workload
add a tolerations section in spec:
spec:
  tolerations:
  - key: "taint-key"
    operator: "Equal" or "Exist"
    value: "taint-key value"
    effect: "NoSchedule" | "PreferNoSchedule" | "NoExecute"

--- change the image repository
1. create the secret to the docker
kubectl create secret docker-registry mydocker --docker-server=<docker repository> --docker-username=youname --docker-passord=<pwd>
2. verify the secret
kubectl get secrets mydocker
3. add the secret to the service account
kubectl patch serviceaccount default -p '{"imagePullSecrets": [{"name": "myregistrykey"}]}'
4. you can export the yaml by
kubectl get serviceaccounts default -o yaml > sa.yaml
then edit with the imagePullSecrets and the name of the secret.
5. apply the yaml by replacing It
kubectl replace serviceaccount default -f sa.yaml
6. verify the effectiveness
kubectl run <image> --image=<image> --restart=Never
kubectl get pod <image> -o=jsonpath='{.spec.imagePullSecrets[0].name}{"\n"}'

--alternatively, you can use your existing docker login credential
1. log into docker
docker login
2. check the config.json file
cat ~/.docker/config.json
3. create a secret based on existing docker credential
kubectl create secret generic dockerregistry --from-file=.dockerconfigjson=<path/to/.docker/config.json> --type=kubernetes.io/dockerconfigjson
kubectl get secret dockerregistry --output yaml

--install network plugin
using Calico 
1. on the master node, apply the calico manifest. 
curl -LO https://docs.projectcalico.org/v3.20/manifests/calico.yaml
or if you want to use the latest version, you can do:
curl -LO https://docs.projectcalico.org/manifests/calico.yaml

2. check the file to ensure the images referred doesn't source k8s.gcr.io
grep image ./calico.yaml
3. apply the file
kubectl apply -f ./calico.yaml
4. watch and ensure the installation successfully
watch kubectl get pods --all-namespaces

-- check and change the CIDR of the cluster:
the CIDR setting for service, cluster, pods can be set as a parameter to the kube-controller-manager, which is part of the 
/etc/kubernetes/manifests/kube-controller-manager.yaml

-- to expand or change the IP pool for various usage, you can use calicoctl command to do so.

--install nginx ngress controller 
1. download the deployment file:
curl -LO https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.1.0/deploy/static/provider/baremetal/deploy.yaml
2. check the docker images used:
grep image ./deploy.yaml
3. change the images from k8s.gcr.io to docker hub, you need to first search from hub.docker.com, and the replace the image.
4. after that, you then reapply the deployment.
kubectl apply -f ./deploy.yaml
5. watch the status
watch kubectl get pods --all-namespaces
it will take sometime to have all the pods Running

--check the log errors from various places
1. from the system journal
journalctl -f -u kubelet --- this will produce a log stream containing the kubelet service
2. check the pod related messages
kubectl describe -n namespace pod <pod-id>
3. check the contain related messages
3.1 first list out the containers(container ID) inside the pod
docker ps -a -f name=<pod-name> --format={{.ID}}
3.2 then check the docker log
docker logs <container id>

Expose services from pods or other resources
kubectl expose pod <pode-name> --name=<svc-name> --type=NodePort --port=<port on cluster> --target-port=<port on the pod>

Commands:
create role and rolebinding
1. kubectl create ns ns-app1
2. kubectl create serviceaccount sa-name -n ns-app1 
3. kubectl create clusterrole deployment-cr -verb=create --resource=deployment,statefulset,daemonset
4. kubectl -n ns-app1 create rolebinding <rolebinding-name> --clusterrole=<deployment-rc> --serviceaccount=<ns-app1:sa-name>

manage node availabilty
1. kubectl cordon <node-name> //this make the node unscheduleable
2. kubectl drain <node-name> --ignore-daemonsets --delete-local-data --force //this drain the workloads off the node
3. kubectl uncordon <node-name>

upgrade kubeadm
-on master node
1. kubectl cordon <master-node>
2. kubectl drain <master-node> --ignore-daemonsets
3. apt-mark unhold kubeadm kubectl kubelet
4. apt-get update && apt-get install -y kubeadm=1.20.1-00 kubelet=1.20.1-00 kubectl=1.20.1-00
5. apt-mark hold kubeadm kubelet kubectl
6. systemctl restart kubelet
7. kubeadm upgrade plan
8. kubeadm upgrade apply v1.20.1 --etcd-upgrade=false
optionally
9. kubectl rollout undo deployment coredns -n kube-system
10. kubectl uncordon <master-node>
11. kubectl get node
upgrade worker node
-on master node
1. cordon the worker node
2. drain the worker node (--ignore-daemonsets)

-on worker node
1. apt-get unmark kubeadm kubelet
2. apt-get update
3. apt-get install kubeadm=1.20.1-00
4. kubeadm upgrade node //<for node, should use "node" instead of "plan" on master>
5. apt-get install kubelet=1.20.1-00
6. systemctl restart kubelet
-on master node
7. kubectl uncordon <worknode>
8. kubectl get node

backup and restore etcd
-- check the file location
on the master node
1. ps -aux | grep apiserver
this will print out the etcdca file, client key file, client cert file to be used calling to etcdctl, as well as the etcd server's endpoint (the port)

or 
1.1 lsof -i :2379 (this is the default port of etcd server), or fuser 2379/tcp, or netstat -lptn | grep 2379
1.2 with the process ID, you then: ps -aux | grep <processid>, it will show the full startup command for the process
1.3 from there you can find out the location of: the cert of the server, the peer cert etc.
1.4 however, you can NOT find the needed etcd client certs ... which is only used in the apiserver

2. ETCDCTL_API=3 etcdctl --endpoints <the etcd endpoint> --cacert=<path to the ca cert of the etcd> --cert=<etcd client's cert file> --key=<etcd client's key file> snapshot save <the path of the backup>
3. you then check the saved file path

- check the kubelet information
1. ps -aux | grep kubelet
this will give information like the 
/usr/bin/kubelet \
  --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf 
  --kubeconfig=/etc/kubernetes/kubelet.conf 
  --config=/var/lib/kubelet/config.yaml 
  --network-plugin=cni 
  --pod-infra-container-image=k8s.gcr.io/pause:3.6
2. check the static Pod install location
cat /var/lib/kubelet/config.yaml | grep staticPod
3. Then place a pod yaml file will create a static Pod on that particular node.

Create User and the configuration:
a> create certificate signing request
1. get the CSR file which is submitted from client
openssl genrsa -out myuser.key 2048 -- this will generate the client key file
openssl req -new -key myuser.key -out myuser.csr -- this will generate the CSR file using the client key file
2. generate the base64 string for the csr. 
cat myuser.csr | base64 | tr -d "\n" -- this will convert the csr into a single line request string to use in the K8S CSR yaml file.
3. create the CertificateSigningRequest with a yaml file
apiVersion: certificates.k8s.io/v1
kind: CertificateSigningRequest
metadata:
  name: myuser
spec:
  request: 
  signerName: kubernetes.io/kube-apiserver-client
  expirationSeconds: 86400  # one day
  usages:
  - client auth
4. apply the CSR in kubernetes cluster
kubectl apply -f <CSR yaml file>
5. you can then check the CSR by:
kubectl get csr -- the status should be pending.
6. approve the CSR
kubectl certificate approve <the CSR name in the yaml>
7. Get the Certificate from the approved CSR
kubectl get csr/myuser -o yaml -- the cert is in the yaml of the file {status.certificate}
kubectl get csr/myuser -o jsonpath='{.status.certificate}' | base64 -d > myuser.crt 
--- the cert file should be return to the applicant (myuser)

Create the role and the role-binding
8. kubectl create role developer --verb=create --verb=get --verb=list --verb=update --verb=delete --resource=pods
9. kubectl create rolebinding developer-binding-myuser --role=developer --user=myuser

Add the user to kubeconfig (it's usually /etc/kubernetes/admin.conf)
10. kubectl config set-credentials myuser --client-key=myuser.key --client-certificate=myuser.crt --embed-certs=true
-- the command will set the credentials for the user to the kubeconfig file and embedding the key and certificate.
Add the context to the cluster
11. kubectl config set-context myuser --cluster=kubernetes --user=myuser
Switch to the new context
12. kubectl config use-context myuser
13. kubectl config current-context -- this will print the current context
14. kubectl config get-contexts -- this will print all the contexts, active one will be marked with *
-- check user auth
1. kubectl auth can-i <verb> <resource> 
kubectl auth can-i list pod 
kubectl auth can-i list pod --as <different user>

-- interactive with container
1. copy files and dirs to/from pod
kubectl cp <file-spec-src in the OS> <some-pod>:<dir> -- this copy files to a directory in the remote pod in default ns
2. copy files and dirs to/from container
kubectl cp <file/dir in OS> <somepod>:<dir> -c <container id> 
kubectl cp <namespace>/<pod-name>:<dir> <local-path>  --- here the namespace is a must
3. execute a command in the container
kubectl exec -it podname -- <command> 
can also specify a container to run the command with by -c 
-i to pass local stdin to container
-t tell container that the stdin is a local TTY
4. check logs from pod or a resource (if a pod only has one container, then the -c is optional)
kubectl logs -f -p <Pod or resource> -c <container>
5. docker ps -a -f name=<pod-name> --format={{.ID}}
6. docker logs <container id>

Kubernetes Reference Documentation:
https://kubernetes.io/docs/reference

Network policy
- check the current network provider support the feature

-- a sample yaml file
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: test-network-policy
  namespace: default
spec:
  podSelector:
    matchLabels:
      role: db
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - ipBlock:
        cidr: 172.17.0.0/16
        except:
        - 172.17.1.0/24
    - namespaceSelector:
        matchLabels:
          project: myproject
          ns: ns-target
    - podSelector:
        matchLabels:
          role: frontend
    ports:
    - protocol: TCP
      port: 6379
  egress:
  - to:
    - ipBlock:
        cidr: 10.0.0.0/24
    ports:
    - protocol: TCP
      port: 5978

-- another one will deny all ingress traffic
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-ingress
spec:
  podSelector: {}
  policyTypes:
  - Ingress

-- one will allow all ingress traffic
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-ingress
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  ingress:
  - {}

-- one will allow all egress traffic
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-ingress
spec:
  podSelector: {}
  policyTypes:
  - Egress
  egress:
  - {}

-- one will deny all egress traffic
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-ingress
spec:
  podSelector: {}
  policyTypes:
  - Egress

-- one will deny all egress and ingress traffic
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-ingress
spec:
  podSelector: {}
  policyTypes:
  - Egress
  - Ingress


kubectl create ns ns-target 
kubectl create ns ns-app1 
kubectl create ns ns-app2
kubectl -n ns-target create deployment nginx-target --image=nginx:1.19.4
kubectl -n ns-app1 create deployment --nginx-app1 --image=nginx:1.19.4
kubectl -n ns-app2 create deployment --nginx-app2 --image=nginx:1.19.4
target:
kubectl exec -it deployment/nginx-target -- /bin/sh
echo "hi, nginx-target" > /usr/share/nginx/html/index.html 
app1: echo "hi, nginx-app1!" > /usr/share/nginx/html/index.html 
app2: echo "hi, nginx-app2!" > /usr/share/nginx/html/index.html

kubectl -n ns-target create deployment image=ubuntu:20.04 -- sleep infinity
kubectl -n ns-app1 create deployment image=ubuntu:20.04 -- sleep infinity
kubectl -n ns-app2 create deployment image=ubuntu:20.04 -- sleep infinity

create 3 deployment for Ubuntu, to test the connectivity:
ubuntu-target -- ubuntu-app1 -- ubuntu-app2 --
kubectl -n ns-target create deployment ubuntu-target --image=ubuntu:20.04 -- sleep infinity

Get svc and endpoints (ep):
kubectl -n ns-target get services
nginx-target -- nginx-app1 -- nginx-app2 --
在 3 个 ubuntu 中分别起 81 端口，python3 -m http.server 81 &
kubectl -n ns-target expose deployment nginx-target --name=svc- nginx-target --port=80
kubectl -n ns-app1 expose deployment nginx-app1 --name=svc- nginx-app1 --port=80
kubectl -n ns-app2 expose deployment nginx-app2 --name=svc- nginx-app2 --port=80