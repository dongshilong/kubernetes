apt-get update

cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
br_netfilter
EOF

cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
sudo sysctl --system

apt-get install -y docker

sudo apt-get install -y apt-transport-https ca-certificates curl

-- check the current config file content.
-- including clusters, contexts, users, current context, preferences etc. 
kubectl config view 

-- cluster management, cluster is a combination of control plane and a set of working node, plus the etcd for parameter storage
1. check cluster information
kubectl config set <key=value> -- set individual key value
kubectl config get-clusters
kubectl config set-cluster --add a new cluster in kubeconfig
kubectl config delete-cluster <cluster>

-- context management, context is setup of default cluster, user, and namespace when no specific set
kubectl config current-context --- display current context
kubectl config get-contexts
kubectl config use-context <context>
kubectl config set-context  --- add a new context
kubectl config delete-context <context> -- delete a context

-- User management
kubectl config get-users
kubectl config set-credentials <add a new user>
kubectl config delete-user <user>

-- create deployment
kubectl create deployment <dep-name> --image=<image id>
-- get the deployment
kubectl get deployments.apps
kubectl get pod
kubectl describe deployments.apps <dep-name> -- you'll find out the container name

-- upgrade the deployment image
kubectl set image <dep-name> <container-name>=<new image id> --record
kubectl describe deployments.apps <dep-name> | grep -i | image

-- upgrade the kubeadm/kubelet for master and worker nodes
1. From master
1.0 check current version
kubeadm version
1.1 check the available version, also include the current version
kubeadm upgrade plan --- this will print out the available version of kubeadm and kubelet, this can only run on master
1.2 upgrade the executable (kubeadm and kubelet)
apt update
apt-mark unhold kubeadm, kubelet
apt install kubeadm=<version> // pls note the version structure: <major>.<minor>.<revision>-<build number: 00>
1.3 apply the upgrade change
-- download the corresponding docker images for the version and label it accordingly.
as folllowing instruction. 
-- apply the version.
kubeadm upgrade apply v<version number> --- please note, this version doesn't include build number (-xx)
--- this will download the required docker images from gcr.io, which doesn't work in China without a VPN, so need to be carefule
1.4 upgrade the CNI plugin when necessary
2. From other control nodes
<download and upgrade the executable>
kubeadm upgrade plan -- check the upgrade availabilty
kubeadm upgrade node --- no need to provide the version, instead of kubeadm upgrade apply
3. Drain the workload from master nodes
kubectl drain <node to drain > --ignore-daemonsets
4. upgrade the kubelet & kubectl version
apt-mark unhold kubelet && apt-get upgrade && apt-get install -y kubelet=<version-00> && apt-mark hold kubelet
apt-mark unhold kubectl && apt-get upgrade && apt-get install -y kubectl=<version-00> && apt-mark hold kubectl
systemctl daemon-reload
systemctl restart kubelet
5. Uncordon the workload
kubectl uncordon <node-to-drain>

--For work nodes
1.upgrade the kubeadm executable
apt-mark unhold kubeamd && apt-get update && apt-get install -y kubeadm=<version-xx> && apt-mark hold kubeadm
2. upgrade the node
kubeadm upgrade node
3. drain the node 
from master
kubectl drain <node to drain>
4. upgrade the kubelet
apt-mark unhold kubelet && apt-get update && apt-get install -y kubelet=<version-xx> && apt-mark hold kubelet
5. reload the services
systemctl daemon-reload
systecmtl restart kubelet
6. uncordon the node
from master
kubectl uncordon <node to drain>

verify the status
from master
kubectl get nodes

Static Pod - static pod will be tied to a specific node, and will NOT be managed by the apiserver
- static pod will automatically restart when kubelet restart
two ways to run static pod
- Filesystem-hosted solution:
kubelet will load yaml files (manefist) from a folder in filesystem in the config file or the manifest path
there are two options for this:
1. by setting in kubelet config file: usually /var/lib/kubelet/config.yaml, then staticPodPath: 
--- the config file is specified in the kubelet command --config, or through
1.1 to identify the config file path, using
ps -aux | grep kubelet, and then find out the path of the --config
1.2 then grep the path by:
cat <config file path from above> | grep -i static
1.3 then create the manifest file to the folder
2. by setting a pod-manifest-path to kubelet. this is not included by default, so if you want to change, need to restart kubelet on the node
--- set the kubelet with --pod-manifest-path parameter
- Web-hosted static pod manifest
kubelet will periodically scan the URL to pick up the manifests from the url. this is done by specify a url to kubelet command
kubelet --manifest-url=<url>

ETCD cluster management
- Run a single-node cluster
etcd --listen-client-urls=http://<private-ip>:2379 --advertise-client-urls=http://<private-ip>:2379
--- then run the APIServer with --etcd-servers=<private-ip>:2379

- Run a multi-nodes cluster
etcd --listen-client-urls=http://<ip1>:2379,http://<ip2>:2379 --advertise-client-urls=http:<ip1>:2379,http:<ip2>:2379
--- then run the APIServer with --etcd-servers=<ip1>:2379,<ip2>:2379

- Run a multi-node cluster with Load-balancer
run the etcd with all the IPs behind the load-balancer
APIServer --listen-client-urls=<lb-ip>:2379

- Secure the ETCD
this includes two parts:
1. peer comms - between ETCD members
1.1 specify --peer-key-file=<peer.key>, --peer-cert-file=<peer.cert> and use HTTPS as comms protocol
2. client comms - between ETCD and the clients (API server in this case, or etcdctl in some case)
3. set trusted CA for client access
--client-cert-auth=<true/false>, this will force the ETCD cluster verify client using system CA or CA passed by trusted-ca-file
--trusted-ca-file=<CA file which signes the client cert>

find out the secure information:
1. findout the etcd pod by:
kubectl get pods -n kube-system
2. describe the pod by:
kubectl describe pod/<etc pod>
3. find out the cert/key and trust ca file as well as the end-points
4. then operate with etcdctl by:
ETCDCTL_API=3 etcdctl --endpoints=<one from listen-client-urls> --cacert=<trusted-ca-file> --cert=<cert-file> --key=<key file> snapshot save <filename>
ETCDCTL_API=3 etcdctl --endpoints=<one from listen-client-urls> --cacert=<trusted-ca-file> --cert=<cert-file> --key=<key file> restore <filename>

Delete resource
kubectl delete pod, service <names> --grace-period=<seconds, -1, 0, 1 (immediate)>, --force (grace-period need to 0)

service
1. create service by expose
1.1 kubectl run pod-web --image=nginx
1.2 kubectl expose pod pod-web --name=web-pod-svc --port=80 --- this will create a svc of default clusterIP type.
1.3 lookup service
--- kubectl exec -it <pod-name> -- nslookup <svc-name> ---- this will lookup by service name
--- kubectl exec -it <pod-name> -- nslookup <svc-end-point-ip, and replace . by ->.default.pod ---- this will lookup the default pod
you can find out the IP address by kubectl get pod -o wide
you can't directly ping to a pod, you have to ping the ip, or the service wrapping the pod. 

1.4 check dns status
--- install dnsutil jessie-dnsutils
first install the docker image, as I'm installing from docker hub, so I need to change the image setting below
then apply the dnsutil pod by 

apiVersion: v1
kind: Pod
metadata:
  name: dnsutils
  namespace: default
spec:
  containers:
  - name: dnsutils
    image: claudiubelu/jessie-dnsutils:1.3
    command:
      - sleep
      - "3600"
    imagePullPolicy: IfNotPresent
  restartPolicy: Always

then can run the nslookup using the newly created pod
-- kubectl exec -it dnsutils -- nslookup svc-name
-- kubectl exec -it dnsutils -- nslookup kubernetes.default  --- this should always work
can also print the local dns setting by dump the local names resolution:
-- kubectl exec -it dnsutils -- cat /etc/resolv.conf
check if DNS pod is running
-- kubectl get pods -n kube-system -l k8s-app=kube-dns
check for DNS logs
-- kubectl logs -n kube-system -l k8s-app=kube-dns

storage
- pv, two ways of provisions of pv
1. static
-- the yaml file:

apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv0003
spec:
  storageClassName: manual
  capacity:
    storage: 5Gi
  accessModes:
    - ReadWriteMany
  hostpath:
    path: /root/k8s/pv

a storageClassName is used to bind the PV to a class, which will later be used by PVC for PV provision. 
a PV without a class can only be used to bind PV to a PVC without class associated, or the dyanamic provision is disabled

the persistent volume claim 

--yaml file
apiVersion: v1
kind: PersistentVolumeClaim
metadata: 
  name: my-PVC
spec:
  storageClassName: manual -- align with previous setting
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 3Gi

apply the PV or PVC in pod
add a volumes section to specs of the pod

volumes:
- name: <volume name>
  persistentVolumeClaim:
    claimName: <the PVC name> 
- name: configmap
  configMap:
     name: configMap-name
     items: 
       - key: <config-key> -- in case there're multiple config files in the configMap
         path: this is the name the file will be mounted to the mountPath, you will need indicate this is the subPath if necessary. 

then in the container's section, add volumeMounts section:
volumeMounts:
- name: <volume name defined in volumes>
  mountPath: <the path in the container>
  subPath: <subpath in local, if the same volume are to be used in multiple volumeMount>

you can also create a volume with configMap, which will map a configMap to a file. 
volumes: 
- volume-name:
  configMap:
    name: name-of-configMap
- volume-name:
  configMap:
    name: name-of-configMap
    items:  -- this is to select a set of keys to be mapped as file, if NO items, all keys will be mapped as file
    - key: key in config-map
      path: "file-name"

then in the container section:
- image: nginx
  name: container-name
  volumeMounts:
  - name: <volume-name defined>
    mountPath: /etc/nginx/nginx.conf
    subPath: nginx.conf -- as the items' section define the key and the path, 
                        so path will be appended to the mountPath, you can specify the subPath to remove the last file name

change the nginx index.html content:
- kubectl exec -n <namespace> pod/<pod>


2. dynamic
- dynamic provision is provided through PVC with a storage class (storageClassName), when a PVC doesn't have storage class, 
dynamic provision is disabled. 
- To enable, cluster admin need to enable DefaultStorageClass admission controller on API Server. 
--- --enable-admission-plugins 

- Create pod with replicas, use deployment than directly run command
kubectl create deployment <dept-name> --image=<imageid> --replica=3
kubectl run <pod-name> --image=<imageid> --- this will create a pod with only one replica
kubectl get deployments.apps --- will get the apps.
kubectl get pods --- this will get the pods

- scheduling
1. drain command with kubectl drain <node> --ignore-daemonsets --- this will drain workload off the node, and the node will become unscheduleable
2. uncordon the node kubectl uncordon <node> --- this will make the node schedulable again
3. tain the node with NoSchedule, the node will be unschedule for workload without appropriate toleranace. 
-- kubectl taint <node> <key>=<value>:NoScheule/PreferNoSchedule/NoExecute
4. Tolerate the node for scheduling
-- add "tolerations" secion to the spec, with key, oprator, value, and effect

debug:
1. kubectl describe pod/svc/deployment/... 
2. kubectl logs pod/pod-name
3. systemctl status kubelet
4. journalctl -f -t kubelet

CSR
1. the CSR yaml
apiVersion: v1
kind: CertificateSigningRequest
metadata:
  name: myuser-csr
spec:
  request: <the request >
  signerName: kubernetes.io/kube-apiserver-client
  experirationSeconds: 86400 # one day
  usages:
  - client auth
2. request is the base64 string of the csr. 
cat myuser.csr | base64 | tr -d "\n"
3. approve the CSR
kubectl certificate approve <my-csr>
4. get/retrieve the cert from the approved CSR - it's a base64-encoded in the .status.certificate field:
kubetl get csr/my-csr -o jsonpath='{.status.certificate}' | base64 -d > myuser.crt 

User:
1. create user with cert/key:
kubectl config set-credentials --client-certificate=<cert-file> --client-key=<key-file>
2. create user with bearer token flags:
kubectl config set-credentials --token=<bearer-token>
3. create user with basic auth flags:
kubectl config set-credentials --username=<user-name> --password=<password>
4. using an external auth provider:
kubectl config set-credentials cluster-admin --auth-provider=<gcp, aws etc>
5. Enable OpenID connect auth provider for cluster-admin, usually the service will provide openid, clientid, client-secret, refresh-token
kubectl config set-credentials cluster-admin \
  --auth-provider=oidc 
  --auth-provider-arg=issuer-url=<issurer-url> \
  --auth-provider-arg=client-id=<client-id> \
  --auth-provider-arg=id-token=<id_token> \
  --auth-provider-arg=client-secret=<client secret> \
  --auth-provider-arg=refresh-token=<refresh token> \
  --auth-provider-arg=idp-certificate-authority=<path to the CA cert>
6. Using --token option:
kubectl --token=<bearer token> commands
7. Using WebHook Token Authentication
webhook auth is a hook for verifying bearer tokens

scheduling
- kubectl cordon node-name -- if you want to drain a node, need to first make it unschedulable
kubectl cordon <node0name>
- drain the pods from the node --- need to ignore daemonsets, and clean the local data and force
kubectl drain <node> --ignore-daemonsets --delete-local-data --force
- need to check node
kubectl get nodes
- uncordon the node
kubectl uncordon <node-name>
then get the nodes

upgrade the kubeadm
1. kubectl get nodes
2. kubectl cordon <master-node>
3. kubectl drain <master-node> --ignore-daemonsets --force
4. apt-mark unhold kubeadm kubelet kubectl
5. apt-get update & apt-get install -y kubeadm=1.20.1-00 kubelet=1.20.1-00 kubectl=1.20.1-00
6. apt-mark hold kubeadm kubectl kubelet
7. kubeadm upgrade plan
8. kubeadm upgrade apply <v1.20.1> --etcd-upgrade=false
9. systemctl restart kubelet, systemctl daemon-reload
10. kubectl uncordon <master-node>

Clean environment:
kubectl delete all --all -n <namespace>
kubectl delete ns <namespace>

Named Ports:
- pod port can specify a port name, which can be used in service as target-port
- similarly service port can also specify a name, and the targetPort can use name port defined in container(pod)
- ingress can also refer to the service port name. 

apiVersion: v1
kind: Pod
metadata:
  name: mypod
  namespace: default
  labels:
    app: named-port-pod
spec:
  containers: 
  - name: c-name1
    image: nginx
    ports:
    - name: port-name
      port: 8080

--- // the service
apiVersion: v1
kind: Service
metadata:
  name: myservice
  namespace: default
spec:
  ports:
  - name: http-svc-pod
    protocol: TCP
    port: 80
    targetPort: port-name
  selector:
    app: named-port-pod
- name: <port-name>
  port: <port-num>
  target-port: <target-port>

-- user request
1. prepare the request string:
cat user.csr | base64 | tr -d "\n"
the result will be provided the the CertificateSigningRequest request .spec.request 
2. create CertificateSigningRequest
apiVersion: certificates.k8s.io/v1
kind: CertificateSigningRequest
metadata: 
  name: username
spec:
  request: <the string from step 1>
  signerName: kubernetes.io/kube-apiserver-client
  usages:
  - client auth
3. apply the file 
kubectl apply -f <yaml file from step2>.yaml
4. approve the CSR from kubernetes:
kubectl certificate approve username
5. check the user's privileges:
kubectl get pod --as username
6. create Role and RoleBinding for the username
kubectl create role role-name -n <ns> --resource=<pods,services,deployments> --verb=get,list,delete,create
7. create the rolebinding
kubectl create rolebinding rolebinding-name -n <ns> --role=role-name --user=user-name
8. then check the permission:
kubectl get pod --as username


Network policy
-- Network policy apply to POD - NOT service. So before you check the connectivity, make sure you're connecting to the pod port/ep
-- check the pod end-point, the following command will print out all POD EPs. 
-- kubeclt get endpoint -n <namespace> or --all-namespaces.
-- you set the POD EP by setting the ports/containerPort

fubar: 

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-port-from-namespace
  namespace: fubar
spec:
  podSelector: {}
  policyTypes: [Ingress]
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: my-app
    ports:
      - protocol: TCP
        port: 80

kubectl label <pod/pod-name> label-key=value --- this will add a new label
kubectl label pod/pod-name label-key-  --- this will delete a label

ingress: 
--- the yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ping
  namespace: ing-internal
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  ingressClassName: nginx
  rules:
  - http:
     paths:
     - path: /hello
       pathType: Prefix
       backend:
         service:
           name: hello
           port:
             number: 5678
--- to test the ingress, need to forward local traffic to ingress-controller
kubectl port-forward -n ingress-nginx svc/ingress-nginx-controller 8080:80
--- please double check the ingress controll's serving port. 
then test the service by:
curl -kL localhost:8080/hello

--- scale the deployment
kubectl scale deployment <develoyment-name> -n <namespace> --replicas=2
kubectl get deployment -n <>

--- schedule a pod to a node, using the spec.nodeSelector like:
spec:
  nodeSelector:
     label: value

--- check node ready status, which is tainted with NoSchedule
kubectl get nodes -o yaml | grep -i taint | grep NoSchedule

--- a pod with multiple container:
apiVersion: v1
kind: Pod
metadata:
  kucc1
spec:
  containers:
  - name: nginx
    image: nginx
  - redis
    image: redis


--- create PV:
apiVersion: v1
kind: PersistentVolume
metadata: 
   name: app-config
spec:
   capacity: 
      storage: 2Gi
   accessModes: [ReadWriteMany]
   hostPath:
      path: /srv/app-config
- PV are volume plugin, but have a lifecycle independent of any individual Pod that uses the PV. 

--- create PVC
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pv-volume
spec:
  accessModes: [ReadWriteOnce] # this mode will NOT be able to use PV as the mode is different
  resources:
    requests:
       storage: 10Mi
  storageClassName: csi-hostpath-sc # this is optional, default to DefaultStorageClass

-- use the PVC
apiVersion: v1
kind: Pod
metadata: 
   name: web-server
spec: 
   containers:
   - name: nginx
     image: nginx
     volumeMounts:
     - mountPath: "/usr/share/nginx/html"
       name: mypv
   volumes:
   - name: mypv
     persistentVolumeClaim:
        claimName: pv-volume

- you can also explicitly bind a PV to a PVC, you need to:
  .spec.storageClassName: "" # have to be empty, otherwise, default storage class will be used. 
  .spec.volumeName: <pv-name> # the name of PV to be binded to the PVC.

-- upgrade the storage and record:
kubectl edit pvc pv-volume --record
and then change the storage: 70Mi

-- storage class
- is a profile defined by admin to specify the attributes (qos, backup policy etc)
- to describe the class of the storage admin offers. 
- key fields:
1. provisioner: can be the cloud storage provider. or Local
2. parameters: provisioner specific.
3. reclaimPolicy: Retain, Delete(default), Recycle (is deprecated)
4. volumeBindingMode: Immediate, WaitForFirstConsumer
5. mountOptions: only apply to PV dynamically created by storageclass. Careful, will fail if provider doesn't support feature. 
6. allowVolumeExpansion: true or false
-- create a local storage class:
apiVersion: storage.k8s.io/v1
kind: storageclass
metadata:
  name: local-storage
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer

-- dynamically storage provisioning
1. need a storage class, which set the class or profile of the storage to be provided. 
   nothing special - all storage class
2. dynamically provisioning
   in PVC: just set the storageClassName, will serve the purpose. 
3. Note that - if PVC do not specify a storageClassName - DefaultStorageClass will be used. 
             - only ONE PVC can use default storage class name. 
             - there should have at most one default storageclass. 


-- local volume storage
- a local volume represents a mounted local storage like disk, partition or directory
- local volume can only be used a static created PV, dynamically provisioning is not supported
- compare to hostPath volumes, local volume is used in a durable and portable manner without manually scheduling pods to node. 
- system is aware the node constrains by checking Affinity attribute of PV. 

-- Example of local volume
apiVersion: v1
kind: PersistentVolume
metadata:
  name: local-pv
spec:
  capacity:
    storage: 2Gi
  volumeMode: Filesystem # can also be Block (raw block device)
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Delete
  storageClassName: local-storage # the volumeBindingMode inside suggests set to WaitForFirstConsumer
  local:
    path: /mnt/disks/ssd1
  nodeAffinity: # this is a must for local volume, scheduler will use this to put the pod to the correct node
    required: 
      nodeSelectorTerms:
      - matchExpressions:
        - key: keyvalue
          operator: In
          values:
          - value1
          - value2

-- Example of hostPath volume
apiVersion: v1
kind: PersistentVolume
metadata: 
  name: hostpath-pv
spec:
  capacity: 
    storage: 100Mi
  accessModes:
  - ReadWriteMany
  hostPath: 
    path: /root/result


--- monitor logs:
kubectl logs pod foobar | grep 'unable-to-access-website' > file

--- create sidecar to monitor the logs:
apiVersion: v1
kind: Pod
metadata: 
  name: 11-factor-app
spec:
  containers:
  - name: 11-factor-app
    image: busybox
    args: [/bin/sh -c, 'i=0; while true; do echo "$i: $(date)" >> /var/log/11-factor-app.log; i=$((i+1)); sleep 1; done']
    volumeMounts:
    - name: varlog
      mountPath: /var/log
  - name: sidecar
    image: busybox
    args: [/bin/sh -c, 'tail -n+1 -f /var/log/11-factor-app.log']  
    volumeMounts:
    - name: varlog
      mountPath: /var/log
  volumes:
  - name: varlog
    emptyDir: {}
-- check the logs:
kubectl logs -f pod/11-factor-app -c sidecar

--- check CPU/Memory Performance
kubectl top pod -l name=cpu-loader -A --sort-by=cpu
-- -l --selector: label
   -A --all-namespaces=true: include all namespaces
   --sort-by: only support CPU and mem. 
   default sort by descending order. 

--- troubleshooting
1. get node status - identify which node is having problem. 
kubectl get nodes
2. check system status of kubelet
systemctl status kubelet
3. enable the service
systemctl enable kubelet
systemctl restart kubelet
4. check system journal log
journalctl -f -t kubelet

-- list pv sort by name:
kubectl get pv --sort-by=.metadata.name 

-- daemonset:
--- default will run on ALL nodes, normal Pod is controlled by scheduler. 
--- node selector (.spec.selector) can select the nodes to run 

-- static Pod:
--- like a normal pod, but created and managed directly by kubelet, can operate via API
--- file place directly on the target node. 
--- useful for cluster bootstrapping. 
--- potentially will deprecate in future. 
steps:
ps -aux | grep kubelet 
find out the --config file path. 
grep static /var/lib/kubelet/config.yaml
then create a yaml file in the directory. 

-- InitContainer 
-- containers will be created first in the pod. which allow us do some preparation work. 
apiVersion: v1
kind: Pod
metadata: 
  name: test-InitContainer

spec:
  volumes:
  - name: myvol
    emptyDir: {}
  containers: 
  - name: checker
    image: busybox
    command: ["/bin/sh -c", "if [ -f /data/checking.txt]; then sleep 1000; else eixt 1; fi"]
    volumeMounts:
    - name: myvol
      mountPath: /data
  initContainers: 
  - name: starter
    image: busybox
    command: ["/bin/sh -c" "touch /data/checking.txt"]
    volumeMounts:
    - name: myvol
      mountPath: /data
-- Upgrade container image, and then roll-back:
kubectl create deployment nginx-app --image=<image>
kubectl rollout history deployment nginx-app -- this will list out all the revisions
kubectl rollout undo deployment nginx-app --to-revision=<number> --- default to previous revision

-- get Pods for a service
1. get service and the pod selector
kubectl get svc -o wide
find out the label in the selector column. then select pod using the label by:
kubectl get pod -l app=demo

-- Mapping a secret to a pod
1. create the secret with password (for example)
apiVersion: v1
kind: Secret
metadata:
  name: mysecret
type: Opaque
# note that data has to be base64 encoded, you can use 
# echo "string" | base64
data: 
  password: bXlwYXNzd29yZAo=
  password2: bXlwYXNzd29yZAo=
2. use the secret in pod:
apiVersion: v1
kind: Pod
metadata:
  name: pod1
spec:
  volumes:
  - name: mypassword
    secret: 
      secretName: mysecret
  containers:
  - name: bb-1
    image: docker.io/dongshilong/busybox-k8s
    volumeMounts:
    - name: mypassword
      mountPath: "/data/" # this will cause k8s create a file for each key in secret
    env:
    - name: DB_PASSWORD
      valueFrom:
        secretKeyRef: 
          name: mysecret
          key: password
    command: ['sh', '-c', 'ls -l /data && env | grep DB_PASSWORD']

the pod will run once and quit (status to Complete), but you check the logs by:
kubectl logs pod/pod1

--Security Context:
- set running user and running group
apiVersion: v1
kind: Pod
metadata:
  name: sc-pod
spec:
  containers:
  - name: sc1
    image: docker.io/dongshilong/busybox-k8s
    command: ["sh", "-c", "sleep 20000"]
    securityContext: # this control the container security context
      capabilities:
        add: [SYS_PTRACE]
  securityContext: # it's a pod level security context
    runAsUser: 1000
    runAsGroup: 2000
    fsGroup: 3000
    fsGroupChangePolicy: "OnRootMismatch"

-- check server settings:
kubectl config view
this is useful to check .kubeconfig file errors. 


-- create pod with special capability:
apiVersion: v1
kind: Pod
metadata: 
  name: pod-cap
spec:
  containers:
  - name: bb
    image: docker.io/dongshilong/busybox-k8s
    securityContext:
      capabilities:
        add: ["SYS_TIME", "SYS_PTRACE"]
find available capabilities:
capsh --print 
all the capabilities are formated as cap_<name to use (but in upper case)>
result like: 
cap_chown,cap_dac_override,cap_dac_read_search,cap_fowner,cap_fsetid,cap_kill,cap_setgid,cap_setuid,cap_setpcap,cap_linux_immutable,cap_net_bind_service,cap_net_broadcast,cap_net_admin,cap_net_raw,cap_ipc_lock,cap_ipc_owner,cap_sys_module,cap_sys_rawio,cap_sys_chroot,cap_sys_ptrace,cap_sys_pacct,cap_sys_admin,cap_sys_boot,cap_sys_nice,cap_sys_resource,cap_sys_time,cap_sys_tty_config,cap_mknod,cap_lease,cap_audit_write,cap_audit_control,cap_setfcap,cap_mac_override,cap_mac_admin,cap_syslog,cap_wake_alarm,cap_block_suspend,cap_audit_read

Using nslookup to check for service and store result in a file outside the pod:
1. create a special container with hostPath to the master node (where you run the command). 
apiVersion: v1
kind: Pod
metadata:
  name: pv-recycler
  namespace: default
spec:
  restartPolicy: Never
  volumes:
  - name: vol
    hostPath:
      path: /root/result
  containers:
  - name: pv-recycler
    image: "docker.io/dongshilong/busybox-k8s"
    command: ["/bin/sh", "-c", "nslookup demo > /result/svc.txt && nslookup 172.18.166.177 > /result/pod.txt && ls -l /result"]
    volumeMounts:
    - name: vol
      mountPath: /result
  nodeSelector: # this is a must, otherwise it very likely it will go nothing.
    kubernetes.io/hostname: master
2. create a running pod with busybox:
kubectl run bb-nslookup --image=docker.io/dongshilong/busybox-k8s -- sleep 20000
3. run nslookup through the pod by:
kubectl exec pod/bb-nslookup -- nslookup <service name> > localfilepath
kubectl exec pod/bb-nslookup -- nslookup <pod-ip> > pod.path 

This can used to test the network policy - by using busybox. 
create a pod and with an TTY, which will automatically delete the pod after exit. 
kubectl run busybox --rm -ti --image:ubuntu -- /bin/sh
kubectl run busybox --rm -ti --image:ubuntu -n test --labels label=value,label2=value -- /bin/sh

Run a pod and also expose it as a service
kubectl run nginx --image:nginx --expose --port 80 -n namespace-test --labels label1=val,label2=val

You can also run a pod with stdin/tty enabled, and you then attach a shell to it.
containers:
- name: shell
  image: busybox
  securityContext:
     capabilities: 
       add:
       - SYS_PTRACE --- so that you can run ps command.
  stdin: true
  tty: true

after you created the pod, you can then attached to it by:
kubectl attach -it <pod-name> -c shell. // here you don't have to provide a command like -- /bin/sh 
you can also check the other container details by:
1. ps -aux -- find out the processes. and identify the pid of the other container
2. check the content with:
   head /proc/8/root/etc/nginx/nginx.conf
3. netstat -ntlp (t: for tcp, l: for listening, p: for program, n: for numeric)
4. check process path:
   ps -eau | grep <ps-name>
   ls /proc/<pid>
  

make sure you check the indicated context from the exam, and change it as requested. 

Performance:
kubectl top node|pod -l|--selector <label> -A|--all-namespaces --sort-by=[cpu|memory]
kubectl top 

nginx:
-- install locations:
/etc/nginx/nginx.conf -- the config location
-- default.conf
/etc/nginx/conf.d/default.conf
-- html location
/usr/share/nginx/html

busybox:  
--- use 1.28.3, as the latest version doesn't have nslookup work. 
--- if you want to use other non-default image, you can use: docker.io/<image-id>, eg:
--- I've create a workale one with: docker.io/dongshilong/busybox-k8s, which has nslookup and curl
so you will need to run:
--- kubectl run --rm -it --image=docker.io/dongshilong/busybox-k8s -n <ns if necessary> -- /bin/sh

check the servic availability:
-- nslookup - <service name>, please remember if you're running a pod without NS specified, you can still lookup by setting the name like:
nslookup <svcname>.<namespace>.svc.cluster.local
-- curl service:<port> -- this is not installed by default.
-- wget -q -O- service:<port> -- this is installed. 


Workload allocation:
1. need to enable the metrics-server service on the cluster. 
kubectl get apiservices - this list out all the current services


-- set the image repository to aliyun mirror
sudo curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg
apt-key add /usr/share/keyrings/kubernetes-archive-keyring.gpg
tee "deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main" >> /etc/apt/sources.list.d/kubernetes.list

apt-get update

-- install the kubeadm and kubelet and kubectl
sudo apt-get install -y kubelet kubeadm kubectl
or sudo apt install kubelet=<version>
-- get installed packages
apt list --installed | grep <packge-name>
-- hold package from update
sudo apt-mark hold kubelet kubeadm kubectl
-- apt update, --- update the available packages


--- set the CGroup Driver
sudo mkdir /etc/docker
cat <<EOF | sudo tee /etc/docker/daemon.json
{
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m"
  },
  "storage-driver": "overlay2"
}
EOF

sudo systemctl enable docker
sudo systemctl daemon-reload
sudo systemctl restart docker

--check the needed docker images for kubeadm
kubeadm config images list
-- result will be something like:
k8s.gcr.io/kube-apiserver:v1.23.1
k8s.gcr.io/kube-controller-manager:v1.23.1
k8s.gcr.io/kube-scheduler:v1.23.1
k8s.gcr.io/kube-proxy:v1.23.1
k8s.gcr.io/pause:3.6
k8s.gcr.io/etcd:3.5.1-0
k8s.gcr.io/coredns/coredns:v1.8.6

-- replace with the equivalent in docker hub
docker login

docker pull 
docker pull k8simage/kube-apiserver:v1.23.1
docker pull k8simage/kube-controller-manager:v1.23.1
docker pull k8simage/kube-scheduler:v1.23.1
docker pull k8simage/kube-proxy:v1.23.1
docker pull k8simage/pause:3.6
docker pull k8simage/etcd:3.5.1-0
docker pull 274658950/coredns:v1.8.6 -- this is because k8simage rep does't has the version.

-- add the tag that'll be recognized by kubeadm
docker tag k8simage/kube-apiserver:v1.23.1 k8s.gcr.io/kube-apiserver:v1.23.1
docker tag k8simage/kube-controller-manager:v1.23.1 k8s.gcr.io/kube-controller-manager:v1.23.1
docker tag k8simage/kube-scheduler:v1.23.1 k8s.gcr.io/kube-scheduler:v1.23.1
docker tag k8simage/kube-proxy:v1.23.1 k8s.gcr.io/kube-proxy:v1.23.1
docker tag k8simage/pause:3.6 k8s.gcr.io/pause:3.6
docker tag k8simage/etcd:3.5.1-0 k8s.gcr.io/etcd:3.5.1-0
docker tag 274658950/coredns:v1.8.6 k8s.gcr.io/coredns/coredns:v1.8.6

-- edit the default image repository, you can change the default configmap to point to a different image repository
1. check the current setting:
kubectl describe cm kubeadm-config -n kube-system
2. check the change the imageRepository: k8s.gcr.io, to a workable image repository
3. you can save the file in yaml format by:
kubectl get cm kubeadm-config -n kube-system -o yaml > kubeadmin-config.yaml
4. then edit and re-apply the yaml
-- now start the kubeadm
kubeadm init

It will now show the command for other node to join the cluster.

--- run command to get the tokens
kubeadm token list


-- print the join command
kubeadm token create --print-join-command

-- or if you want to do the detail work, first:
1. get the discovery token ca-cert hash string
openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //'
2. then issue the join command as:
kubeadm join --discovery-t0ken <token-name from token list command> --discovery-token-ca-cert-hash sha256:<the hash string>

-- alternatively, you can join by using the following command, if the token doesn't have CA pinned
kubeadm <endpoint url> --token <the token from the kubeadm token list> --discovery-token-unsafe-skip-ca-verification

-- install the same docker images on each node. 

-- once you got the token, you should also get the endpoint of the master. you then join to the master from the working node
kubeadm join 

-- reset the environment , this can be done when the environment is having problem, and can't successfully init.
kubeadm reset

-- set up the Kubernetes environment
export KUBECONFIG=/etc/kubernetes/admin.conf

-- troubleshooting
1. show status of the system pods
kubectl get pods -n kube-system -o wide
-- if any of the pods status isn't in Running status, then you can check the error by

2. show pod error status
kubectl describe pod <the node name> -n kube-system

3. show logs of the pod
kubectl logs <pod name> -n kube-system

4. show system journals with the kubernetes
journalctl -f -u kubelet
this will use tailed format of the logs to kubelet (like tail -f)
-f: follow, follow the latest journal
-n: show n lines
-u: show only logs for a service
--since: "yyyy-mm-dd hh:MM:ss" --until ""
--since 1 hour ago

4. after fixed the problems
systemctl daemon-reload
systemctl restart docker
systemctl restart kubelet (or systemctl restart kubelet.service)
systemctl status kubelet

-- set container/pod security context
kubectl run <pod-name> --image=<image> --dry-run=client -o yaml 
then change the yaml file and add the securityContext section in spec
securityContext:
  runAsUser: <userid>
  runAsGroup: <gid>
  fsGroup: <additional groupid>

-- scheduling
1. mark the node unschedulable
kubectl cordon <nodename>
kubelctl drain <nodename> --force --ignore-daemonsets   --- this will force the pods/workload to be evicated from the node
2. mark the node scheduleable again
kubectl uncordon <nodename>
3. Add taint infor for node to unschedulable for certain attributes
kubectl taint node <nodename> key=value:effect -- this will make the node unschedule only if the pod has the explicit tolerance
--effect here can be: NoSchedule, PreferNoSchedule, NoExecute
kubectl taint node <nodename> key:effect -- this will add a taint with no value. 
4. Remove the taint
kubectl taint node <nodename> key:effect-  -- this will clear the taint (the key and the effect), and the node will be schedule for all workloads
kubectl taint node <nodename> key-  --- this will clear all taint with the key. 
5. Create tolerance, a tolerance will allow a tainted node to accept a workload
add a tolerations section in spec:
spec:
  tolerations:
  - key: "taint-key"
    operator: "Equal" or "Exist"
    value: "taint-key value"
    effect: "NoSchedule" | "PreferNoSchedule" | "NoExecute"

--install network plugin
using Calico 
1. on the master node, apply the calico manifest. 
curl -LO https://docs.projectcalico.org/v3.20/manifests/calico.yaml
or if you want to use the latest version, you can do:
curl -LO https://docs.projectcalico.org/manifests/calico.yaml

2. check the file to ensure the images referred doesn't source k8s.gcr.io
grep image ./calico.yaml
3. apply the file
kubectl apply -f ./calico.yaml
4. watch and ensure the installation successfully
watch kubectl get pods --all-namespaces

-- check and change the CIDR of the cluster:
the CIDR setting for service, cluster, pods can be set as a parameter to the kube-controller-manager, which is part of the 
/etc/kubernetes/manifests/kube-controller-manager.yaml

-- to expand or change the IP pool for various usage, you can use calicoctl command to do so.

--install nginx ngress controller 
1. download the deployment file:
curl -LO https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.1.0/deploy/static/provider/baremetal/deploy.yaml
2. check the docker images used:
grep image ./deploy.yaml
3. change the images from k8s.gcr.io to docker hub, you need to first search from hub.docker.com, and the replace the image.
4. after that, you then reapply the deployment.
kubectl apply -f ./deploy.yaml
5. watch the status
watch kubectl get pods --all-namespaces
it will take sometime to have all the pods Running

--troubleshooting
0. from the resource:
kubectl logs pod/<pod-name>
1. from the system journal
journalctl -f -u kubelet --- this will produce a log stream containing the kubelet service
2. check the pod related messages
kubectl describe -n namespace pod <pod-id>
3. check the container related messages
3.1 first list out the containers(container ID) inside the pod
docker ps -a -f name=<pod-name>  #this will list the command and the container ID.
docker ps -a -f name=<pod-name> --format={{.ID}}
3.2 then check the docker log # this is same as 
docker logs <container id>
kubectl logs pod/pod-name -c <container-name>

-- display pods by age, this sort descending order
kubectl get pod --all-namespaces --sort-by='{.status.startTime}'
-- display the pods by age ascending order, NR = 1, leave the header unchanged. 
kubectl get pod --all-namespaces --sort-by=.status.startTime | awk 'NR==1; NR>1 {print $0 | tac}'
Expose services from pods or other resources
kubectl expose pod <pode-name> --name=<svc-name> --type=NodePort --port=<port on cluster> --target-port=<port on the pod>

Commands:
create role and rolebinding
1. kubectl create ns ns-app1
2. kubectl create serviceaccount sa-name -n ns-app1 
3. kubectl create clusterrole deployment-cr -verb=create --resource=deployment,statefulset,daemonset
4. kubectl -n ns-app1 create rolebinding <rolebinding-name> --clusterrole=<deployment-rc> --serviceaccount=<ns-app1:sa-name>

manage node availabilty
1. kubectl cordon <node-name> //this make the node unscheduleable
2. kubectl drain <node-name> --ignore-daemonsets --delete-local-data --force //this drain the workloads off the node
3. kubectl uncordon <node-name>

upgrade kubeadm
-on master node
1. kubectl cordon <master-node>
2. kubectl drain <master-node> --ignore-daemonsets
3. apt-mark unhold kubeadm kubectl kubelet
4. apt-get update && apt-get install -y kubeadm=1.20.1-00 kubelet=1.20.1-00 kubectl=1.20.1-00
5. apt-mark hold kubeadm kubelet kubectl
6. systemctl restart kubelet
7. kubeadm upgrade plan
8. kubeadm upgrade apply v1.20.1 --etcd-upgrade=false
optionally
9. kubectl rollout undo deployment coredns -n kube-system
10. kubectl uncordon <master-node>
11. kubectl get node
upgrade worker node
-on master node
1. cordon the worker node
2. drain the worker node (--ignore-daemonsets)

-on worker node
1. apt-get unmark kubeadm kubelet
2. apt-get update
3. apt-get install kubeadm=1.20.1-00
4. kubeadm upgrade node //<for node, should use "node" instead of "plan" on master>
5. apt-get install kubelet=1.20.1-00
6. systemctl restart kubelet
-on master node
7. kubectl uncordon <worknode>
8. kubectl get node

backup and restore etcd
-- check the file location
on the master node
1. ps -aux | grep apiserver
this will print out the etcdca file, client key file, client cert file to be used calling to etcdctl, as well as the etcd server's endpoint (the port)

or 
1.1 lsof -i :2379 (this is the default port of etcd server), or fuser 2379/tcp, or netstat -lptn | grep 2379
1.2 with the process ID, you then: ps -aux | grep <processid>, it will show the full startup command for the process
1.3 from there you can find out the location of: the cert of the server, the peer cert etc.
1.4 however, you can NOT find the needed etcd client certs ... which is only used in the apiserver

2. ETCDCTL_API=3 etcdctl --endpoints <the etcd endpoint> --cacert=<path to the ca cert of the etcd> --cert=<etcd client's cert file> --key=<etcd client's key file> snapshot save <the path of the backup>
3. you then check the saved file path

- check the kubelet information
1. ps -aux | grep kubelet
this will give information like the 
/usr/bin/kubelet \
  --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf 
  --kubeconfig=/etc/kubernetes/kubelet.conf 
  --config=/var/lib/kubelet/config.yaml 
  --network-plugin=cni 
  --pod-infra-container-image=k8s.gcr.io/pause:3.6
2. check the static Pod install location
cat /var/lib/kubelet/config.yaml | grep staticPod
3. Then place a pod yaml file will create a static Pod on that particular node.

Create User and the configuration:
a> create certificate signing request
1. get the CSR file which is submitted from client
openssl genrsa -out myuser.key 2048 -- this will generate the client key file
openssl req -new -key myuser.key -out myuser.csr -- this will generate the CSR file using the client key file
2. generate the base64 string for the csr. 
cat myuser.csr | base64 | tr -d "\n" -- this will convert the csr into a single line request string to use in the K8S CSR yaml file.
3. create the CertificateSigningRequest with a yaml file
apiVersion: certificates.k8s.io/v1
kind: CertificateSigningRequest
metadata:
  name: myuser
spec:
  request: 
  signerName: kubernetes.io/kube-apiserver-client
  expirationSeconds: 86400  # one day
  usages:
  - client auth
4. apply the CSR in kubernetes cluster
kubectl apply -f <CSR yaml file>
5. you can then check the CSR by:
kubectl get csr -- the status should be pending.
6. approve the CSR
kubectl certificate approve <the CSR name in the yaml>
7. Get the Certificate from the approved CSR
kubectl get csr/myuser -o yaml -- the cert is in the yaml of the file {status.certificate}
kubectl get csr/myuser -o jsonpath='{.status.certificate}' | base64 -d > myuser.crt 
--- the cert file should be return to the applicant (myuser)

Create the role and the role-binding
8. kubectl create role developer --verb=create --verb=get --verb=list --verb=update --verb=delete --resource=pods
9. kubectl create rolebinding developer-binding-myuser --role=developer --user=myuser

Add the user to kubeconfig (it's usually /etc/kubernetes/admin.conf)
10. kubectl config set-credentials myuser --client-key=myuser.key --client-certificate=myuser.crt --embed-certs=true
-- the command will set the credentials for the user to the kubeconfig file and embedding the key and certificate.
Add the context to the cluster
11. kubectl config set-context myuser --cluster=kubernetes --user=myuser
Switch to the new context
12. kubectl config use-context myuser
13. kubectl config current-context -- this will print the current context
14. kubectl config get-contexts -- this will print all the contexts, active one will be marked with *
-- check user auth
1. kubectl auth can-i <verb> <resource> 
kubectl auth can-i list pod 
kubectl auth can-i list pod --as <different user>

-- interactive with container
1. copy files and dirs to/from pod
kubectl cp <file-spec-src in the OS> <some-pod>:<dir> -- this copy files to a directory in the remote pod in default ns
2. copy files and dirs to/from container
kubectl cp <file/dir in OS> <somepod>:<dir> -c <container id> 
kubectl cp <namespace>/<pod-name>:<dir> <local-path>  --- here the namespace is a must
3. execute a command in the container
kubectl exec -it podname -- <command> 
can also specify a container to run the command with by -c 
-i to pass local stdin to container
-t tell container that the stdin is a local TTY
4. check logs from pod or a resource (if a pod only has one container, then the -c is optional)
kubectl logs -f -p <Pod or resource> -c <container>
5. docker ps -a -f name=<pod-name> --format={{.ID}}
6. docker logs <container id>

Kubernetes Reference Documentation:
https://kubernetes.io/docs/reference


-- autocompletion for kubernetes
1. install the bash-completion
apt-get install -y bash-completion
2. check if the bash-completion installed successfully
type _init_completion
3. if _init_completion can't be resolved, try add the following to ~/.bashrc
source /usr/share/bash-completion/bash_completion
4. add the kubectl autocompletion scripts to auto-completion (bash completion) by
source <(kubectl completion bash) # this will enable auto-completion with kubectl, but NOT with alias
5. applying the autocompletion to alias of kubectl 
complete -F __start_kubectl k # this add the alias function k to __start_kubectl

-- using the tmux, shell terminal multiplexier. 
1. enter the tmux
tmux
2. Enable mouse
tmux set-option -g mouse on
3. Navigating through the window (pane)
tmux select-pane -L (left), -R (right), -D(down), -U(up)
4. split the window
tmux split-window -h(horizentally) -v(vertically)
